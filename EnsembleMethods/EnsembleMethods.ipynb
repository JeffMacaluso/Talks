{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "**TODO: Mention continuation of presentation**\n",
    "\n",
    "**TODO: Link slides**\n",
    "\n",
    "## Overview\n",
    "\n",
    "**TODO: Mention using scikit-learn for basic algorithms, hyperparameter tuning them, then using additional frameworks**\n",
    "\n",
    "## Setup\n",
    "\n",
    "**TODO: Talk about what is being set up and why**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T17:23:10.814658Z",
     "start_time": "2018-07-04T17:23:08.205443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018/07/04 12:23\n",
      "OS: win32\n",
      "Python: 3.5.5 | packaged by conda-forge | (default, Apr  6 2018, 16:03:44) [MSC v.1900 64 bit (AMD64)]\n",
      "NumPy: 1.12.1\n",
      "Pandas: 0.23.1\n"
     ]
    }
   ],
   "source": [
    "# TODO: Trim out any of these that aren't used\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "print(time.strftime('%Y/%m/%d %H:%M'))\n",
    "print('OS:', sys.platform)\n",
    "print('Python:', sys.version)\n",
    "print('NumPy:', np.__version__)\n",
    "print('Pandas:', pd.__version__)\n",
    "\n",
    "# Formatting for seaborn plots\n",
    "sns.set_context('notebook', font_scale=1.1)\n",
    "sns.set_style('ticks')\n",
    "\n",
    "# Displays all dataframe columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an artificial data set with [scikit-learn's make_classification function](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)\n",
    "\n",
    "**Note: The dataset is being kept small to keep within time limits for the talk, but increase the size or swap it out with a real-world dataset to get a better idea of how these algorithms compare under different scenarios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T17:23:12.314566Z",
     "start_time": "2018-07-04T17:23:12.142702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.609606</td>\n",
       "      <td>-0.405804</td>\n",
       "      <td>1.646726</td>\n",
       "      <td>-1.350422</td>\n",
       "      <td>1.028112</td>\n",
       "      <td>0.150770</td>\n",
       "      <td>0.628511</td>\n",
       "      <td>1.589373</td>\n",
       "      <td>-0.470484</td>\n",
       "      <td>-1.568524</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.422521</td>\n",
       "      <td>1.471273</td>\n",
       "      <td>-2.185650</td>\n",
       "      <td>-0.368431</td>\n",
       "      <td>-1.657457</td>\n",
       "      <td>0.528370</td>\n",
       "      <td>-1.002433</td>\n",
       "      <td>-1.864973</td>\n",
       "      <td>-1.004933</td>\n",
       "      <td>1.438955</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.908927</td>\n",
       "      <td>-0.049227</td>\n",
       "      <td>0.826947</td>\n",
       "      <td>-0.195883</td>\n",
       "      <td>1.220820</td>\n",
       "      <td>-1.676695</td>\n",
       "      <td>0.720308</td>\n",
       "      <td>0.209854</td>\n",
       "      <td>0.561905</td>\n",
       "      <td>0.758860</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.563904</td>\n",
       "      <td>1.131092</td>\n",
       "      <td>-1.771599</td>\n",
       "      <td>-0.056129</td>\n",
       "      <td>-2.047874</td>\n",
       "      <td>2.180386</td>\n",
       "      <td>-1.217147</td>\n",
       "      <td>-0.923479</td>\n",
       "      <td>0.660668</td>\n",
       "      <td>-0.379916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.353001</td>\n",
       "      <td>1.391851</td>\n",
       "      <td>-1.782492</td>\n",
       "      <td>0.991630</td>\n",
       "      <td>-1.272597</td>\n",
       "      <td>0.234084</td>\n",
       "      <td>-0.772074</td>\n",
       "      <td>-1.587041</td>\n",
       "      <td>-0.552018</td>\n",
       "      <td>1.347231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -1.609606 -0.405804  1.646726 -1.350422  1.028112  0.150770  0.628511   \n",
       "1  1.422521  1.471273 -2.185650 -0.368431 -1.657457  0.528370 -1.002433   \n",
       "2  0.908927 -0.049227  0.826947 -0.195883  1.220820 -1.676695  0.720308   \n",
       "3 -0.563904  1.131092 -1.771599 -0.056129 -2.047874  2.180386 -1.217147   \n",
       "4  1.353001  1.391851 -1.782492  0.991630 -1.272597  0.234084 -0.772074   \n",
       "\n",
       "          7         8         9  label  \n",
       "0  1.589373 -0.470484 -1.568524      1  \n",
       "1 -1.864973 -1.004933  1.438955      1  \n",
       "2  0.209854  0.561905  0.758860      1  \n",
       "3 -0.923479  0.660668 -0.379916      1  \n",
       "4 -1.587041 -0.552018  1.347231      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an artificial dataset to test algorithms on\n",
    "data = datasets.make_classification(n_samples=5000,\n",
    "                                    n_features=10,\n",
    "                                    n_redundant=5,  # Superfluous features working as noise for the algorithms\n",
    "                                    flip_y=0.5,  # Introduces additional noise\n",
    "                                    n_classes=2,\n",
    "                                    random_state=46)\n",
    "\n",
    "# Assigning features/labels to variables for ease of use\n",
    "X = data[0]\n",
    "y = data[1]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=46)\n",
    "\n",
    "# Putting into a dataframe for viewing\n",
    "df = pd.DataFrame(X)\n",
    "df['label'] = y\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Talk about creating this function for gathering results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T17:23:14.658176Z",
     "start_time": "2018-07-04T17:23:14.611303Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data frame for gathering results \n",
    "results = pd.DataFrame(columns=['Accuracy', 'LogLoss', 'AUC', 'TrainingTime'])\n",
    "tuned_results = results.copy()\n",
    "\n",
    "# Function for training a model and retrieving the results\n",
    "def train_model_get_results(model, model_name):\n",
    "    '''\n",
    "    Trains a model and appends the results to the results dataframe\n",
    "    \n",
    "    Input:\n",
    "        - model: The model with specified hyperparameters to be trained\n",
    "        - model_name: The name of the model to be used as the index\n",
    "        - is_tuned: A binary flag for if hyperparameter tuning has been performed\n",
    "    \n",
    "    Output: The results dataframe with the model results added\n",
    "    \n",
    "    Note: Only works with scikit-learn models\n",
    "    '''\n",
    "    \n",
    "    # Collecting training time for results\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('Training the model')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_training_time = end_time - start_time\n",
    "    print('Completed')\n",
    "    \n",
    "    # Calculating the testing set accuracy with the score method\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    \n",
    "    # Calcuating the log loss with predicted probabilities\n",
    "    class_probabilities = model.predict_proba(X_test)\n",
    "    log_loss = metrics.log_loss(y_test, class_probabilities)\n",
    "    auc = metrics.roc_auc_score(y_test, class_probabilities[:, 1])\n",
    "    \n",
    "    # Adding the model results to the results dataframe\n",
    "    model_results = [accuracy, log_loss, auc, total_training_time]\n",
    "    results.loc[model_name] = model_results\n",
    "    \n",
    "    print('\\n', 'Non-tuned results:')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "It's always useful to have a baseline to compare against. I like to use linear/logistic regression due to them being extremely fast to train.\n",
    "\n",
    "**TODO:** Say something about wanting to be better than random chance or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T17:23:16.251831Z",
     "start_time": "2018-07-04T17:23:16.189333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model\n",
      "Completed\n",
      "\n",
      " Non-tuned results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>LogLoss</th>\n",
       "      <th>AUC</th>\n",
       "      <th>TrainingTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.696</td>\n",
       "      <td>0.604543</td>\n",
       "      <td>0.743498</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy   LogLoss       AUC  TrainingTime\n",
       "Logistic Regression     0.696  0.604543  0.743498      0.015625"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating the model\n",
    "logistic_regression = linear_model.LogisticRegression()\n",
    "\n",
    "# Using our user defined function to train the model and return the results\n",
    "train_model_get_results(model=logistic_regression, model_name='Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T21:52:49.624668Z",
     "start_time": "2018-06-30T21:52:49.593513Z"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "Bagging (bootstrap aggregating) is the technique that aggregates models built with bootstrapping, or sampling with replacement, via a majority vote or by averaging the predictions. The trees are independent of each other and can be built in parallel. \n",
    "\n",
    "Bagging models tend to decrease variance.\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "The most popular bagging algorithm is the **random forest**. This algorithm works by building a series of decision trees where each tree uses a random selection of variables, and then decision trees vote on the final answer.  \n",
    "\n",
    "More specifically, for each tree:\n",
    "\n",
    "- Use a different training sample with replacement (bootstrapping) for the data\n",
    "- For each node, choose a number of random attributes and find the best split\n",
    "- Typically is not pruned in order to have a smaller bias\n",
    "\n",
    "Once these trees are grown, a majority vote among all of the trees will be used to make predictions.\n",
    "\n",
    "The main ideas here are that the randomness makes a set of diverse models that helps improve accuracy and using random subsets of features at each split helps make it more efficient.\n",
    "\n",
    "<img src=\"http://www.globalsoftwaresupport.com/wp-content/uploads/2018/02/ggff5544hh.png\">\n",
    "\n",
    "**Advantages:**\n",
    "- Robustness against over-fitting\n",
    "    - Since the model is created through dense randomness, the generalization is typically better, and you can usually increase the accuracy with the number of trees up until a saturation point\n",
    "- Soft thresholding (boundaries) on the instance space.\n",
    "- Able to parallelize training multiple trees at once and thus speed up training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T17:23:19.111030Z",
     "start_time": "2018-07-04T17:23:18.673557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model\n",
      "Completed\n",
      "\n",
      " Non-tuned results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>LogLoss</th>\n",
       "      <th>AUC</th>\n",
       "      <th>TrainingTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.604543</td>\n",
       "      <td>0.743498</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.655333</td>\n",
       "      <td>1.374621</td>\n",
       "      <td>0.693786</td>\n",
       "      <td>0.187489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy   LogLoss       AUC  TrainingTime\n",
       "Logistic Regression  0.696000  0.604543  0.743498      0.015625\n",
       "Random Forest        0.655333  1.374621  0.693786      0.187489"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest = ensemble.RandomForestClassifier(n_jobs=-1)  # n_jobs=-1 uses all available cores\n",
    "\n",
    "train_model_get_results(random_forest, model_name='Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting methods train a sequence of weak learners (a learner that is barely better than random chance) where each successive model focuses on the parts that the previous model got wrong. The trees have to be built in a sequence and generally cannot be built in parallel without clever tricks.\n",
    "\n",
    "Boosting models tend to decrease bias.\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "**TODO: Explain gradient boosting, how it works, and its advantages**\n",
    "\n",
    "<img src=\"https://littleml.files.wordpress.com/2017/03/boosted-trees-process.png\">\n",
    "\n",
    "*Source: [BigML](https://blog.bigml.com/2017/03/14/introduction-to-boosted-trees/)*\n",
    "\n",
    "**Disadvantages:**\n",
    "- Typically overfits easier than bagging\n",
    "- Sensitive to noise & extreme values\n",
    "- Has to be built sequentially, so cannot parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T17:23:23.798247Z",
     "start_time": "2018-07-04T17:23:22.845179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model\n",
      "Completed\n",
      "\n",
      " Non-tuned results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>LogLoss</th>\n",
       "      <th>AUC</th>\n",
       "      <th>TrainingTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.604543</td>\n",
       "      <td>0.743498</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.655333</td>\n",
       "      <td>1.374621</td>\n",
       "      <td>0.693786</td>\n",
       "      <td>0.187489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosted Trees</th>\n",
       "      <td>0.723333</td>\n",
       "      <td>0.596558</td>\n",
       "      <td>0.733223</td>\n",
       "      <td>0.906195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Accuracy   LogLoss       AUC  TrainingTime\n",
       "Logistic Regression     0.696000  0.604543  0.743498      0.015625\n",
       "Random Forest           0.655333  1.374621  0.693786      0.187489\n",
       "Gradient Boosted Trees  0.723333  0.596558  0.733223      0.906195"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_boosting = ensemble.GradientBoostingClassifier()\n",
    "\n",
    "train_model_get_results(gradient_boosting, model_name='Gradient Boosted Trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on interpretability\n",
    "\n",
    "It's possible to obtain \"feature importance\" from both bagging and boosting methods. These are not as interpretable as coefficients from linear/logistic regressions, but can still give us an idea of what is happening. \n",
    "\n",
    "Note that the multicollinearity assumption applies here - these interpretations will be misleading if the features are heavily correlated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T17:23:30.188485Z",
     "start_time": "2018-07-04T17:23:29.594770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ba907d9ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def feature_importance(model):\n",
    "    '''\n",
    "    Plots the feature importance for an ensemble model\n",
    "    '''\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, sorted_idx)\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "feature_importance(random_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "**TODO: Explain stacking, how it works, and its advantages/disadvantages**\n",
    "\n",
    "**TODO: Include a picture for stacking**\n",
    "\n",
    "**TODO: Check if there's a better way to do stacking w/ scikit-learn**\n",
    "\n",
    "**TODO: Check if there's a way to avoid repeating fit/predict lines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T17:23:42.000269Z",
     "start_time": "2018-07-04T17:23:35.250676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model\n",
      "Completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>LogLoss</th>\n",
       "      <th>AUC</th>\n",
       "      <th>TrainingTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.604543</td>\n",
       "      <td>0.743498</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.655333</td>\n",
       "      <td>1.374621</td>\n",
       "      <td>0.693786</td>\n",
       "      <td>0.187489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosted Trees</th>\n",
       "      <td>0.723333</td>\n",
       "      <td>0.596558</td>\n",
       "      <td>0.733223</td>\n",
       "      <td>0.906195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stacking</th>\n",
       "      <td>0.650667</td>\n",
       "      <td>1.065496</td>\n",
       "      <td>0.679866</td>\n",
       "      <td>6.265244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Accuracy   LogLoss       AUC  TrainingTime\n",
       "Logistic Regression     0.696000  0.604543  0.743498      0.015625\n",
       "Random Forest           0.655333  1.374621  0.693786      0.187489\n",
       "Gradient Boosted Trees  0.723333  0.596558  0.733223      0.906195\n",
       "Stacking                0.650667  1.065496  0.679866      6.265244"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_stacking_get_results(list_of_models):\n",
    "    '''\n",
    "    TODO: Write docstring\n",
    "    '''\n",
    "    # The meta learner is the one that takes the outputs from\n",
    "    # the other models as input before final classification\n",
    "    meta_learner = linear_model.LogisticRegression()\n",
    "\n",
    "    # Collecting training time for results\n",
    "    start_time = time.time()\n",
    "    print('Training the model')\n",
    "\n",
    "    # Fitting the first layer models\n",
    "    for model in list_of_models:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    # Collecting the predictions from the models for training\n",
    "    model_output = []\n",
    "\n",
    "    for model in list_of_models:\n",
    "        class_probabilities = model.predict_proba(X_train)[:, 1]\n",
    "        model_output.append(class_probabilities)\n",
    "\n",
    "    # Re-shaping before passing to the meta learner\n",
    "    X_train_meta = np.array(model_output).transpose()\n",
    "\n",
    "    # Fitting the meta learner\n",
    "    meta_learner.fit(X_train_meta, y_train)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print('Completed')\n",
    "\n",
    "    # Collecting the predictions from the models for testing\n",
    "    model_output = []\n",
    "\n",
    "    for model in list_of_models:\n",
    "        class_probabilities = model.predict_proba(X_test)[:, 1]\n",
    "        model_output.append(class_probabilities)\n",
    "\n",
    "    # Re-shaping before passing to the meta learner\n",
    "    X_test_meta = np.array(model_output).transpose()\n",
    "\n",
    "    # Collecting the accuracy from the meta learner\n",
    "    accuracy = meta_learner.score(X_test_meta, y_test)\n",
    "\n",
    "    # Calcuating the log loss with predicted probabilities\n",
    "    class_probabilities = meta_learner.predict_proba(X_test_meta)\n",
    "    log_loss = metrics.log_loss(y_test, class_probabilities)\n",
    "    auc = metrics.roc_auc_score(y_test, class_probabilities[:, 1])\n",
    "\n",
    "    model_results = [accuracy, log_loss, auc, total_time]\n",
    "\n",
    "    results.loc['Stacking'] = model_results\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Adding extra imports for additional models\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "\n",
    "# Defining the learners for the first layer\n",
    "model_1 = linear_model.LogisticRegression()\n",
    "model_2 = ensemble.RandomForestClassifier(n_jobs=-1)\n",
    "model_3 = neighbors.KNeighborsClassifier(n_jobs=-1)\n",
    "model_4 = ensemble.GradientBoostingClassifier()\n",
    "model_5 = svm.SVC(probability=True)\n",
    "\n",
    "# Putting the models in a list to iterate through in the function\n",
    "models = [model_1, model_2, model_3, model_4, model_5]\n",
    "\n",
    "# Running our function to build a stacking model\n",
    "train_stacking_get_results(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "There two main methodologies for hyperparameter tuning: \n",
    "1. Manually testing hypotheses on how changing certain hyperparameters will impact the performance of the model\n",
    "2. Automatically checking a bunch of different combinations of hyperparameters using either a grid search or a randomized search\n",
    "\n",
    "We will start with the second option by using a randomized search and then see if we have any hypotheses for further improving the model.  **TODO: Take this out if the second doesn't actually happen**\n",
    "\n",
    "Grid search makes more intuitive sense, but research from [James Bergstra and Yoshua Bengio](http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf) have shown that random search tends to converge to good hyperparameters faster than grid search. Here's a graphic from their paper that gives an intuitive example of how random search can potentially cover more ground when there are hyperparameters that aren't as important:\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/923/1*ZTlQm_WRcrNqL-nLnx6GJA.png\">\n",
    "\n",
    "*Source: [James Bergstra & Yoshua Bengio](http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf)*\n",
    "\n",
    "## Hyperparameters & Decision Tree Structure\n",
    "\n",
    "Because both random forests and gradient boosted trees use decision trees for their underlying structures, their hyperparameters are largely the same. Here's a recap of the decision tree structure and a quick summary of what each of the hyperparameters we'll be tuning are:\n",
    "\n",
    "<img src=\"http://3.bp.blogspot.com/-2OYbpGM5AsA/T9_UacNCyxI/AAAAAAAAADI/KWb2PJamTVU/s400/1.png\">\n",
    "\n",
    "*Source: [Murtuza Morbiwala](http://insightfromdata.blogspot.com/2012/06/decision-tree-unembellished.html)*\n",
    "\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "This is list is not all-inclusive, but has most of the common hyperparameters:\n",
    "\n",
    "- **Number of Estimators:** The number of decision trees to be trained\n",
    "    - A higher number typically means better predictions (at the cost of computational power) up until a saturation point where the model begins to overfit\n",
    "- **Max Depth:** How deep a tree can be\n",
    "    - This should ideally be low for gradient boosting and large (or none) for random forests\n",
    "- **Minimum Samples per Split:** The minimum samples considered to split a node\n",
    "    - A higher number typically results in better performance at the cost of computational efficiency\n",
    "- **Minimum Samples per Leaf:** The minimum number of samples required to be a leaf node\n",
    "    - A lower number could potentially result in more noise being captured\n",
    "- **Max Features:** The number of features to consider when looking for the best split\n",
    "    - A lower number typically reduces variance/increases bias and improves computational efficiency\n",
    "- **Max Leaf Nodes:** The maximum number of leaf nodes for the tree\n",
    "    - A smaller number could help prevent overfitting\n",
    "- **Learning Rate (gradient boosting only):** The adjustment/step size for each iteration\n",
    "    - A larger step size can help get better performance in fewer iterations, but will plateau at a lower performance\n",
    "    - A smaller step size will require more iterations (number of estimators) but will ultimately achieve a better performance\n",
    "\n",
    "Here is a more visual version of these hyperparameters on a tree: \n",
    "\n",
    "<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2016/02/tree-infographic.png\">\n",
    "\n",
    "*Source: [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)*\n",
    "\n",
    "### General Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Incorporate these into the above**\n",
    "\n",
    "https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters-Tuning.rst\n",
    "\n",
    "For Faster Speed\n",
    "\n",
    "    Use bagging by setting bagging_fraction and bagging_freq\n",
    "    Use feature sub-sampling by setting feature_fraction\n",
    "    Use small max_bin\n",
    "    Use save_binary to speed up data loading in future learning\n",
    "    Use parallel learning, refer to Parallel Learning Guide\n",
    "\n",
    "For Better Accuracy\n",
    "\n",
    "    Use large max_bin (may be slower)\n",
    "    Use small learning_rate with large num_iterations\n",
    "    Use large num_leaves (may cause over-fitting)\n",
    "    Use bigger training data\n",
    "    Try dart\n",
    "\n",
    "Deal with Over-fitting\n",
    "\n",
    "    Use small max_bin\n",
    "    Use small num_leaves\n",
    "    Use min_data_in_leaf and min_sum_hessian_in_leaf\n",
    "    Use bagging by set bagging_fraction and bagging_freq\n",
    "    Use feature sub-sampling by set feature_fraction\n",
    "    Use bigger training data\n",
    "    Try lambda_l1, lambda_l2 and min_gain_to_split for regularization\n",
    "    Try max_depth to avoid growing deep tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tune_get_results(model, parameters, model_name, num_rounds=30):\n",
    "    '''\n",
    "    TODO: Fill docstring\n",
    "    '''\n",
    "    \n",
    "    print('Default Parameters:', '\\n')\n",
    "    print(model, '\\n')\n",
    "    \n",
    "    random_search = RandomizedSearchCV(model,\n",
    "                                       param_distributions=parameters,\n",
    "                                       n_iter=num_rounds, n_jobs=-1,\n",
    "                                       return_train_score=True, random_state=46,\n",
    "                                       verbose=20)  # Prints status of each completed fit\n",
    "    \n",
    "    print('Beginning hyperparameter tuning')\n",
    "    start_time = time.time()\n",
    "    random_search.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    total_training_time = end_time - start_time\n",
    "    print('Completed')\n",
    "    \n",
    "    # Calculating the testing set accuracy on the best estimator with the score method\n",
    "    accuracy = random_search.best_estimator_.score(X_test, y_test)\n",
    "    \n",
    "    # Calcuating the log loss with predicted probabilities\n",
    "    class_probabilities = random_search.best_estimator_.predict_proba(X_test)\n",
    "    log_loss = metrics.log_loss(y_test, class_probabilities)\n",
    "    auc = metrics.roc_auc_score(y_test, class_probabilities[:, 1])\n",
    "    \n",
    "    # Adding the model results to the results dataframe\n",
    "    model_results = [accuracy, log_loss, auc, total_training_time]\n",
    "    tuned_results.loc[model_name] = model_results\n",
    "    \n",
    "    # Plotting the mean training accuracy from the different iterations\n",
    "    sns.distplot(random_search.cv_results_['mean_test_score'])\n",
    "    plt.title('Mean test score')\n",
    "    \n",
    "    print('Best estimator:', '\\n')\n",
    "    print(random_search.best_estimator_)\n",
    "    \n",
    "    print()\n",
    "    print('Accuracy before tuning:', results.loc[model_name]['Accuracy'])\n",
    "    print('Accuracy after tuning:', tuned_results.loc[model_name]['Accuracy'])\n",
    "    \n",
    "    print('\\n', 'Tuned results:')\n",
    "    return tuned_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T23:13:57.881984Z",
     "start_time": "2018-06-30T23:13:57.866361Z"
    }
   },
   "source": [
    "## Baseline\n",
    "\n",
    "For our logistic regression model, we're just going to tune the regularization parameter. One of the advantages of simpler models like this is that they are easier to tune because we don't have nearly as many hyperparameters to worry about.\n",
    "\n",
    "**Note: The number of rounds is being kept small in these examples to keep within time limits for the talk, but increase them in a real-world scenario for more effective hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T17:23:57.874307Z",
     "start_time": "2018-07-04T17:23:53.515196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Parameters: \n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False) \n",
      "\n",
      "Beginning hyperparameter tuning\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  15 | elapsed:    3.5s remaining:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  15 | elapsed:    3.5s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  15 | elapsed:    3.5s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:    3.5s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  15 | elapsed:    3.5s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    3.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    3.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Best estimator: \n",
      "\n",
      "LogisticRegression(C=7.8383235080575062, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "\n",
      "Accuracy before tuning: 0.696\n",
      "Accuracy after tuning: 0.696\n",
      "\n",
      " Tuned results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\statsmodels\\nonparametric\\kde.py:488: RuntimeWarning: invalid value encountered in true_divide\n",
      "  binned = fast_linbin(X, a, b, gridsize) / (delta * nobs)\n",
      "C:\\Anaconda3\\lib\\site-packages\\statsmodels\\nonparametric\\kdetools.py:34: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  FAC1 = 2*(np.pi*bw/RANGE)**2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>LogLoss</th>\n",
       "      <th>AUC</th>\n",
       "      <th>TrainingTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.696</td>\n",
       "      <td>0.604539</td>\n",
       "      <td>0.7435</td>\n",
       "      <td>3.859142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy   LogLoss     AUC  TrainingTime\n",
       "Logistic Regression     0.696  0.604539  0.7435      3.859142"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ba907c3ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = {'C': scipy.stats.uniform(0, 10)}  # Uniform distribution between 0 and 10\n",
    "\n",
    "hyperparameter_tune_get_results(model=logistic_regression, parameters=parameters,\n",
    "                                model_name='Logistic Regression', num_rounds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "**TODO: Explain random forest tuning strategies**\n",
    "\n",
    "Use [this](https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/) as reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T17:24:52.761609Z",
     "start_time": "2018-07-04T17:24:04.452035Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Parameters: \n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False) \n",
      "\n",
      "Beginning hyperparameter tuning\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   23.4s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:   23.4s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:   25.1s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   34.4s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  15 | elapsed:   36.0s remaining:   24.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  15 | elapsed:   36.3s remaining:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  15 | elapsed:   39.8s remaining:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:   42.0s remaining:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  15 | elapsed:   42.8s remaining:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   43.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   43.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Best estimator: \n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=10, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=452, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\n",
      "Accuracy before tuning: 0.655333333333\n",
      "Accuracy after tuning: 0.721333333333\n",
      "\n",
      " Tuned results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>LogLoss</th>\n",
       "      <th>AUC</th>\n",
       "      <th>TrainingTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.604539</td>\n",
       "      <td>0.743500</td>\n",
       "      <td>3.859142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.721333</td>\n",
       "      <td>0.600526</td>\n",
       "      <td>0.733113</td>\n",
       "      <td>47.309644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy   LogLoss       AUC  TrainingTime\n",
       "Logistic Regression  0.696000  0.604539  0.743500      3.859142\n",
       "Random Forest        0.721333  0.600526  0.733113     47.309644"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAENCAYAAAAMmd6uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VOW9x/HPZLKHJITsJJAAgYd9XxRwx63WWrWUKmpptdZq7Wbt7WIr9d5ut1Z729pWW60LWhdcq+KGVpRN2ZfAwxaWhCSEANnIPnP/mMFGFDIkmZxk5vt+vfIic+acM98ckt8885xznsfl9XoREZHQF+F0ABER6R4q+CIiYUIFX0QkTKjgi4iECRV8EZEwoYIvYcsY09cYk9IF+xncFXlEgi3S6QASHowxXqAZyLbWVh733IfAZCDdWnuwG2PtAC4CVnV0B8aYCcBioF9XhRIJFrXwpTtVA7PbLjDGDAeMM3FI7YJ9JAPRXbAfkaBTC1+609PAXOCvbZZdCywEvnJsgTEmF/gjcAa+N4n7gHustV5/F8wfgJlAJrAb+La19k1jzNn+df8FXA+0AAustbcfH8QYs97/7RJjzNestY8bY74GfB/IAD4EbrbW7jDGRAC/B+YALmAtcAtwFFgExBpjaoGR1tq9bV7jU7fz79MF/Aj4BpAErAC+Zq3da4xJB+4GLgZa/T/P7dbaKmPMPOBr/pcYDpwPFAH3+NdvBh4HfmqtbT7J/4WEIbXwpTstBCYZY/IA/EXvauCxYysYY9z4CtxeIBe4ELgRuM6/yv8CccAofIXyVXxvAMeMBDxAf3yF9jvGmNOOD2KtHef/9kx/sb8C+G/gS/jeSN4GXjHGRAGXA+fh+ySSA+wH7rLW7sdXZOustX3aFnu/T93O/9yN+Ir9xfg+aWxrcxyeBxKBYcBoYADwYJv9Tvdnzcf3JvII0Me//lTgTODHx//MImrhS3eqBl7G18r/JXAWvsJe1GadyfgLl7+Fut0Ycw++AvkIcAfQCDQBeUAVvmLa1i/82y4xxuzy729FO9m+BvzBWrvW//jXxphvAWf7X2MA8FX8nx6stZ4Aft6TbXc1cJ+1dhOAMebHwDD/CeAZwABr7RH/c98Fthhjkv3bHrLWvuZ/LhO4FMix1lYD1caYnwMPAT8PIKOEEbXwpbstwFfwwded88hxz+cBsUCFMeaIMeYI8Fv+U9T7Ay8BB/z7msjHf4/rrLV1bR43E9jveR7wk2Ov6X/dvkCetfYt4JvAFcBmYKsx5nPt7bCd7TKBfW3WrbHWrvYvb7HWFrfZ1W7/vwP8/5YdlxugsE3up4F+xpjYAH5uCSMq+NLdFgGZ/m6WzwDPHPf8fqDSWtv32BcwCF9/PsBTwGtAmrX2dNp0B3XSfuAHx73ueOBxY8wgYK21dia+7pcHgaeNMTEn22E72xXj67I6tm6qMea3/hyRxpgBbXY1GPAC5f7HbUc83O9/nNsmdy4wylrb0LFDIaFKBV+6lb+r5Wng78Db1tqa41ZZCRw0xtxpjInxn8B8Efgf//NJwFFrrccYU4Cviyeqg3Ga/PsDeBj4rjFmpDHGZYy5CtiEr1V9LvCC/9xDLXAEX/dUM77upegTtKZPtt1jwM3GmOH+8wQ/AyZba/cAbwL/579PIBX4HfCatbbi+BfwfxJ4G7jHGNPHGJMI/MP/JfIxKvjihAX4Tro+evwT/jeES4BJ+FrBW/Cd0LzFv8r1wC3GmBrgFf++Ijp489ODwMvGmJuttQuAe/GdMK0GfghcYa3dhq94PovvPEANvv7+z/v74zcAq/G9SY07bv8n2+5RfFcivQpU4PsUc6yray6+K4AssB1fF85cTuwqfCeyd+A7JxKB74S1yMe4NB6+iEh4UAtfRCRMtHtZpjHmBnxXGhwzCF//4wv4bvaIA56y1t4RlIQiItIlTqlLxxgzCl+hPxdYiu866n34+lJ/b61dFIyQIiLSead649Vf8N3BNxjYbq0tAjDGLMA3RsopF3z/JWpTgFJ8t5GLiMjJuYFs4ENrbWOgGwVc8I0xs4A4a+0z/kvWSts8XUqba4pPso/5wJ2BvqaIiJzUGcD7ga58Ki38r+Prswffyd62fUEufOOXnJS1dj4wv+0yY8wQYMfjjz9OVlbWKcQREQlPZWVlzJ07Fz7e8G5XQAXfGBONr79+nn9RMb6PE8dk4bvjryNaAbKyssjNbfdDgoiI/McpdYMH2sIfC2xrM0bJSsD473QswjcQ1EOn8sIiItK9Ar0OfzC+Vj0A/jE65uG7i7AQ2Ipv6FsREemhAmrhW2ufxjf+Sdtli4HjbyUXEZEeSnfaioiECRV8EZEwoYIvIhImVPBFRMKECr6ISJjQJOYiJ/HEyr1OR3DE1dMGOh1BgkAtfBGRMKGCLyISJlTwRUTChAq+iEiYUMEXEQkTKvgiImFCBV9EJEyo4IuIhAkVfBGRMKGCLyISJlTwRUTChAq+iEiYUMEXEQkTKvgiImFCBV9EJEyo4IuIhImAJkAxxlwK3AkkAG9Ya79tjJkF3APEAU9Za+8IXkwREemsdlv4xpjBwF+BzwNjgYnGmIuBh4DLgBHAFP8yERHpoQLp0rkcXwu+2FrbDMwBjgLbrbVF1toWYAEwO4g5RUSkkwLp0ikAmowxLwEDgZeBzUBpm3VKgdz2dmSMmY+va0hERLpZIAU/EjgTOBuoBV4C6gFvm3VcgKe9HVlr5wPz2y4zxuQDRQHkEBGRTgik4JcBb1lrKwCMMc/j675pbbNOFrC/6+OJiEhXCaTgvww8YozpC9QAFwMLgR8aYwrwtc6vxncSV0REeqh2T9paa1cC/wu8DxQCe4C/APOAZ/3LtuJ7ExARkR4qoOvwrbUP8ckW/GJgXJcnEhGRoNCdtiIiYUIFX0QkTKjgi4iECRV8EZEwoYIvIhImVPBFRMKECr6ISJhQwRcRCRMq+CIiYUIFX0QkTKjgi4iECRV8EZEwoYIvIhImVPBFRMKECr6ISJhQwRcRCRMq+CIiYUIFX0QkTKjgi4iECRV8EZEwoYIvIhImVPBFRMJEZCArGWPeATKAZv+irwNDgDuAKOD31tr7gpJQRES6RLsF3xjjAoYBedbaFv+yHOBJYBLQCCwzxrxjrS0MZlgREem4QFr4xv/vG8aYVOBvQA3wtrX2EIAxZiHwBeCuoKQUEZFOC6TgpwCLgVvxdd/8G3gKKG2zTikwtb0dGWPmA3eeakgREem8dgu+tXY5sPzYY2PMg8A9wP+0Wc0FeALY13xgfttlxph8oCiQsCIi0nHtXqVjjJlpjDmvzSIXsBvIbrMsC9jftdFERKQrBdKl0xe4yxgzHV+XzpeBa4AFxph0oA64ErgxaClFRKTT2m3hW2tfBl4B1gKrgYestUuBnwDvAOuAJ6y1HwQzqIiIdE5A1+Fba38K/PS4ZU8ATwQjlEhvU9PQTPHheooPH+VATSNHjjZT3dBMc6uHllYvEREuYiMjiI+OJLVPNOl9YshNiSM/NYH4mID+DEU6Tb9pIh1UWlXPxuIqbHkNpVUNH3suMsJFUlwUCdGRRLpdeDxe6ptbOVTXRFn1x9fNTo5lTE4yY3KSSe0T050/goQZFXyRU9DU4mH13sOs2n3ooyLvjnBRkNGHQWkJ5PaNIys5loSYSCJcrk9s7/V6qWlo4UBNI3sO1VF0sI49B4/yRlU5bxSWU5Deh+kFqQzLTPzU7UU6QwVfJAD1Ta28v+MgK3ZVUt/cSoQLRmQnMWFAX4Zm9iEm0h3QflwuX8s/KS6Kgow+H+27sLSKNXuPsKOilh0VtWQmxXDhyCxMViIuFX7pIir4IifR3Oph+c5K/r3tAA3NHuKj3Zw7PINpg/qRGBvVJa8RF+1mUl4/JuX1o7Sqnve3H2TdviM8umIP+anxfG58DllJsV3yWhLeVPBFTuDtreX8/q1tHD7aTFyUm4tHZzFtUCrRkcEbZDY7OY7Zkwdw5rB0Xt9cxtayGu57ewdnDkvnHJNOpFsD3ErHqeCLHKeippGfvbiJRZvKiHDBGQVpnG0yiIsOrNumK2QmxXLd6flsLa3mxfX7eccewJZVc9XUgTqxKx2mgi/Sxuuby/jxcxuprGticl4K0wvSHO1OGZ6dRH5aAq9uLGXVnsP86Z0dXDExlzE5yY5lkt5Lnw9FgIbmVn703Ea+/thqahpb+OlnR/L010/vEX3nsVFurpiYy+xJuXi98M8P9rJ4azler9fpaNLLqIUvYW/3wTpufnwNhaXVDM9K5I9XTWBoZqLTsT5hwsAU+veN49Hlu1m85QCHapu4fGIOkRFqt0lg9JsiYW3Jtgo+96f3KSyt5qqpA3jhlhk9stgfk5kUy01nDWFAShxr9x3h8RV7aW5td6BaEUAFX8KU1+vlH0uLmPePD2ho9vDbL4zlV1eMJTaq+07MdlRibBQ3nDGYYZl9sOU1PLZ8D00tKvrSPhV8CTsej5e7Xi7k5/8qpF9CDE9+/TRmTx7gdKxTEuWO4JppeYzISmRHRS2Prtitlr60S334EpAnVu51OkKXaPF4eHZ1MeuLq8hIjGHe9Hy2ltawtbTG6WinLNIdwVXTBvLkB/soLK3mqQ/3cdXUgbgjdGeufDq18CVsNLa08tjyPawvrmJgv3huPHMwfeOjnY7VKZEREcyZMoDBaQkUllbz4roSXb0jJ6SCL2GhrrGFB98vYvuBWoZnJfLVGYOIjw6ND7hR7giuOS2P/n1jWbXnMO9uq3A6kvRQKvgS8qrrm3lgyS6KD9czcWAKc6flBXV4BCfERrn58un5JMdF8UZhOZtKqpyOJD1QaP3Wixynur6Zv7+/i4raRs4oSOPKiTkh28edGBvFdafnEe2O4JnV+9h/pN7pSNLDqOBLyKpu8BX7g7VNnDk0nYtGZ4X8UMPZyXF8cXIuza1envhgL/VNrU5Hkh5EBV9CUnVDM39/r8hf7NO4cFRmyBf7Y0b2T+Zsk86huiaeXVOsk7jyERV8CTk1Dc08+F4RB2sbOWNoGheOCv2W/fHOG57JIP+VO0t3HHQ6jvQQKvgSUo76r8Y51md/URgWe/BNu/ilKQPoExPJa5vLKFF/vqCCLyGksbmVh5fv5kBNIzOGpIZFn/3JJMZGMXtyLh4vPL1qn+7ElcDvtDXG3A2kWWvnGWPGA38HkoAlwE3W2pYgZRRpV3Orh8dW7vno0svPjMkO62J/zNCMRE4fnMryXZW8tqmMS8f1dzqSOCigFr4x5jzgy20WLQC+aa0dBriArwUhm0hAWj1envpwH7sq6hiZncTlE3JU7Nu4aHQW6YkxLN9VyY4DtU7HEQe1W/CNMf2AXwC/9D/OA+KstSv8qzwMzA5WQJGT8Xq9vLC2hMLSaganJzBnyoCQvc6+o6LcEXxx0gAiXPD82mIaW3SpZrgKpIV/P/AT4LD/cX+gtM3zpUBuF+cSaZfX62XRpjJW7z1Mbkoc107LI0qTfH+qnJQ4zhiazuGjzbxZWO50HHHISfvwjTE3APustYuNMfP8iyOAthf2uoCAzgYZY+YDd556TJFPendbBe/vOEh6YgxfPj2fmF4wlr2Tzh2eweb9VSzfWcnYnGQGpiY4HUm6WXsnbecA2caYdUA/oA++Yp/dZp0sYH8gL2atnQ/Mb7vMGJMPFAWUVsRvZVElbxSW0zcuiq/OGERCTGgMhBZMUe4ILp+Qy9/e28Vza0u49dyh6v4KMyf9/GutPd9aO9paOx74GfCStfYrQIMxZoZ/tWuBRUHOKfKRjSVVvLRuPwnRbr46YxDJcVFOR+o1BqUlMCW/HwdqGlm2UzdkhZuOdnjOBe41xmzF1+r/Q9dFEjmxooN1PLNqH1GREcybMYi0xBinI/U6F47MJD7azeItB6iqb3Y6jnSjgD8HW2sfxndFDtba9cDU4EQS+XTl1Q08tmI3Hq+Xa6flkdM3zulIvVJ8TCQXjcriubUlvLKxlKunDnQ6knQTXdIgvcKRo008vGw3Dc0erpyYy9CMRKcj9WoT81IY2C+eTSVV7KzQtfnhQgVferz6plYeXrabqvpmLhqVxYSBKU5H6vUiXC4uHeu763bRplI8GlEzLKjgS4/W3OrhsRV7OFDTyOlDUjljaJrTkUJGTkoc4wf0Zf+RBtbvO+J0HOkGKvjSY3m8Xp5etY/dlXWMzknmEo2P0+UuGJlJZISLNwrLaWrR4GqhTgVfeiSv18vLG0rZvL+aQWkJzJ6US4SKfZfrGx/NjII0quqbdZlmGFDBlx5pyfaDrNhVSWZSDNdoyISgOmtYOgnRbv69rYKaBl2mGcr0VyQ9ztq9h3l9cxnJcVHMmz6IuGgNmRBMsVFuzhuRSVOLh8VbDzgdR4JIBV96lG3lNTy7ppjYqAjmTc/XXbTdZEp+P9L7xPBh0SHKqxucjiNBooIvPUbJ4XqeWLmXCJeL607LJzMp1ulIYcMd4eLi0Vl4gdc2lTkdR4JEBV96hMraRh5evpvmVg9fnDyA/DSN5NjdTFYig9ISsOU1rNp9yOk4EgQq+OK42sYWHl62m7rGFj47rj+jc5KdjhSWXC4XF4zMBODet7Y5nEaCQQVfHNXY0sqjy3dTWdfEWcPSOX1wqtORwlpeagJDM/qwdEclK3ZVOh1HupgKvjim1ePlnx/s9U883vej1qU4a9YI3//DPW9uw6shF0KKCr44wuv18sK6EraV1zIssw+XT8jVXbQ9xIB+8Zw3PIMPig6xbKda+aFEBV8c8daWclbvOUxO3ziumjpQMy/1MN89fxgAv3vDqpUfQlTwpdutLKrkHVtBv4Rovjw9n5hI3VjV04zOSebCUZms2XuEd7dVOB1HuogKvnSrwv3/mZ7wK9Pz6aO5aHus78zytfLvVV9+yFDBl26zp7KOJz/cR5Q7gi9Pzye1j6Yn7MlGZCdxyZhs1hdX8baGXAgJKvjSLcqrG3h0+R48Xi9XTR1Ibkq805EkAN+ZNRSXy3ddvlr5vZ8KvgRdVX0zDy/bTX1zK5dPyMVkaXrC3mJoZiKfGZPNppJq9eWHABV8CaqG5lYe8U9PeMHITCblaXrC3uaWswsA+PM7Ox1OIp2lgi9B0+LxsGDlHsqqG5g2qB9nDUt3OpJ0wMj+SZw7PIMPdh/igyKNsdObBVTwjTF3GWMKjTGbjTHf8y+bZYzZYIzZboz5n+DGlN7G6/Xy/JoSdlXUMSI7iUvH9deNVb3YLef4Wvl/emeHw0mkM9ot+MaYs4BzgbHAZOBWY8w44CHgMmAEMMUYc3Ewg0rvsnjrAdbuO0JuShxzJg/Q9IS93KS8FE4fnMqSbRVsKNaE571VuwXfWvsucI61tgXIACKBvsB2a22Rf/kCYHZQk0qvsXrPId7eeoCU+CiuOz2f6Ej1HIaCY6189eX3XgH9JVprm40xPwcKgcVAf6C0zSqlQG7Xx5PeZseBWp5fW0JclJt50wfpxqoQMqMglXED+vLa5jK2l9c4HUc6IOCml7X2TiAdGAAMA9pelOsCPO3twxgz3xjjbfsFFJ1iZumhyqoaeHzlHlwuF9eclkd6om6sCiUul4tvHmvl/1ut/N4okD784caY8QDW2qPAc8DZQHab1bKA/e3ty1o731rravsFDOpQculRquqbeWT5bhpbPMyelMsgzVgVks4bnoHJTOSl9fvZW3nU6ThyigJp4Q8G/maMiTHGROM7UXs/YIwxBcYYN3A1sCiIOaUHa2j2TWJSVd/MhaOyGJvb1+lIEiQRES5uPmcIrR4vf12iVn5vE8hJ21eBV4C1wGpgmbX2SWAe8Cy+fv2twMLgxZSeqtXj5ckP91Ja1cDU/H6cOTTN6UgSZJeMyWZgv3gWri7mQE2D03HkFAR0Rs1aOx+Yf9yyxcC4ro8kvcmiTaUfTWKia+3DQ6Q7ghvPHMwdL2zi4aW7+cFFw52OJAHS9XLSYcdmRMpIjOFLUzSJSTj5wqRc0vpE89jyPVQ3NDsdRwKkgi8dsrOilpfWlxAf7ea60/OJjdIkJuEkNsrNV2YMoqaxhSdW7nU6jgRIBV9OWWVtI0+s3IsLF3On5dEvIdrpSOKAa07Lo09MJA++X0RDc6vTcSQAKvhySuqbWnl0+R7qm1u5bHx/XX4ZxpLjopg7bSAVNY08v7bE6TgSABV8CZjH6+XpVfuoqG1kZkEak/P7OR1JHPbVmYOIdkfwwJJdtHo0QUpPp4IvAVu8pRxbXsPQjD5cNDrL6TjSA2QmxXLFxByKDtbx+uYyp+NIO1TwJSCF+6t4x1bQLyGaOVM0+qX8x41nDsblgr++u1PTIPZwKvjSrh0HanlmdTFRbhdzpw0kPloDosl/DE7vw0WjsthQXMWynZVOx5GTUMGXk6ppaObGx1bR2OLhiom5ZCfHOR1JeqCbzhoC+Fr50nOp4MsJeTxebnt6Pbsq6phZkMY4jZEjJzBuQF+mD0nlve0H2Vhc5XQcOQEVfDmhvy7ZyRuF5UwfksqFo3SSVk7uG2f7W/kaVK3HUsGXT7VyVyV3v27JSorlj1dN0LAJ0q6ZBWmM6p/Eoo2l7D5Y53Qc+RQq+PIJB2sbufWfa3G5XPzp6gmk9tFEJtI+l8vFN84egscLD7y3y+k48ilU8OVjWj1evvvUOg7UNHL7hUY3V8kpuXh0Nnmp8SxcVcyBag2d3NOo4MvH3PfODt7bfpBzh2dw4xmDnY4jvYw7wsWNZw6mqdXDQ0t3Ox1HjqOCLx9ZtuMg9761jf7Jsfxu9jgi1G8vHXDlxFzS+sTw+AoNndzTqOALABU1jXzryXW4XS7+NHciKRoBUzooNsrNV2fmU9PYwuMrNHRyT6KCL77r7Z9Zz8HaRv7rouFMHJjidCTp5a45LY/EmEgeWqqhk3sSFXzhoaVFLNlWwVnD0rl+5iCn40gISIqN4urTfEMnP7dGQyf3FCr4YW5TSRW/eW0raX2iuVv99tKFrp9xbOjknRo6uYdQwQ9jR5ta+NaTa2lu9XL37HGkJ+p6e+k6GUmxXDkph92VR3ltk4ZO7glU8MPYz18qZFdFHdfPHMTZJsPpOBKCbjxziIZO7kFU8MPUKxtKeWrVPkZmJ/GDi4zTcSREDUpL4OLRWWwsqWLpDg2d7LSABjY3xtwJfNH/8BVr7Q+MMbOAe4A44Clr7R1ByihdrLSqnh89t4G4KDd/uGoCMZFupyNJCLvprCG8urGMv767k5lD05yOE9babeH7C/sFwARgPDDJGHMV8BBwGTACmGKMuTiYQaVreDxebn9mA9UNLdzx2REUZPRxOpKEuLG5fZlRkMr7OzR0stMC6dIpBW6z1jZZa5uBLcAwYLu1tsha2wIsAGYHMad0kUeX7+b9Hb6hE66eOtDpOBImvnFWAaAJUpzWbpeOtXbzse+NMUPxde38Ed8bwTGlQG57+zLGzAfuPOWU0iV2HKjhV4u2khIfxa+vHINL89JKN5lRkMronCRe3VRK0cE6BqUlOB0pLAV80tYYMwp4E7gd2AW0PeXuAjzt7cNaO99a62r7BehOn27Q3Orhu0+tp7HFw6+uGENGYqzTkSSMuFwuvnFWAV4vPKAJUhwTUME3xswAFgM/tNY+AhQD2W1WyQL2d3086Sp/fHsHG0uquGJiDheNzm5/A5EudtHoLPJT43l2dQn7j9Q7HScsBXLSdgDwAnC1tfZJ/+KVvqdMgTHGDVwNLApeTOmMtXsPc987O8jpG8f8z41yOo6EKXeEi1vOKaCp1cNf/q1WvhMCaeF/H4gF7jHGrDPGrAPm+b+eBQqBrcDCIGWUTjja1ML3nl6Px+u7mzYpNsrpSBLGLp+QQ15qPE99uI/SKrXyu1sgJ22/DXz7BE+P69o40tV+9epWig7WccPMQZw+JNXpOBLmIt0RfPOcAm5fuIG//Hsnd1022ulIYUV32oawf9sDPLZiD8My+/D9C3U3rfQMx1r5T36gVn53U8EPUYfrmvjBwg1EuV3cO2c8sVG6m1Z6hmOtfPXldz8V/BDk9Xq544VNHKhp5DuzhjGqf7LTkUQ+Rq18Z6jgh6AX1+3nlY2lTMpL4aazhjgdR+QTIt0RumLHASr4IWb/kXp++uIm4qPd3PPFcbg1oYn0UJdPyGFgP18rv6yqwek4YUEFP4R4PF6+/8x6ahpa+OlnR5KXqtvXpeeKckfwzXN9rfz73tnhdJywoIIfQh5aWsSynZWcNzyDL00Z4HQckXZdPiGH/NR4nvxwL3sq65yOE/JU8EOELavhf1+3pCZE8+srx2pgNOkVotwRfP9C45tm841tTscJeSr4IaCxpZVvP7mWphYPv7lyrOamlV7lkjHZjMtN5l/r97Oh+IjTcUKaCn4IuOeNbWwtq+GqqQOYNTLT6Tgip8TlcvFfFw8H4NeLtmru2yBSwe/lVuyq5IH3dpGfGs8dl4x0Oo5Ih0wfksZZw9JZtrOSJdsPOh0nZKng92LVDc3c9vR6Ilwu7pkznoSYgKYoFumR/uui4bhcvla+x6NWfjCo4Pdid764mZIj9dxyTgETB6Y4HUekU0b2T+Ly8TlsKa3mxfUlTscJSSr4vdTLG/bz/NoSxuUmc+u5BU7HEekS3z1/GNHuCO5+fRuNLa1Oxwk5Kvi9UFlVAz95fhOxURHcO2c8UW79N0poGNAvnutOz6PkSD2PLtvjdJyQo0rRy3g8Xm57Zh1V9c385JKRDE7v43QkkS51yzkFJMdF8YfF26moaXQ6TkhRwe9l/vLuTpbu8N1Ne820gU7HEelyKQnR3HbBMGoaW7j7det0nJCigt+LrNp9iHve3EZWUiy/nT1Od9NKyLp66kCGZyXy9Op9rN+nm7G6igp+L3HkaBPf+udavF4vf7hqAv0Sop2OJBI0ke4I7rx0FF4v3PnSZl2m2UVU8HsBr9fL95/ZwP6qBr4zaxhTB/VzOpJI0J0+JJVLxmazbt8R/vnhXqfjhAQV/F7g4WW7eWtLOdOHpHLLOboEU8LHzz47ksSYSH6zaKtO4HYBFfweblNJFb96dSupCdH8fs54TWgiYSUzKZbvX2iobmjhF68UOh2n1wu44Btjkowxm4wx+f7Hs4wxG4wx240x/xO0hGHsUF0TX39sNU2tHn5QLg5zAAANuklEQVT3xXFkJMU6HUmk211zWh5jc5N5Yd1+lmyrcDpOrxZQwTfGTAPeB4b5H8cBDwGXASOAKcaYi4MVMhy1tHq49Z9rKDlSz3dmDeVsk+F0JBFHuCNc/OqKMURGuPjRcxupaWh2OlKvFWgL/2vALcB+/+OpwHZrbZG1tgVYAMwOQr6w9dvXLUt3VDJrRAbfOneo03FEHDWqfzI3n1NAyZF6frVoq9Nxeq2Ahle01t4AYIw5tqg/UNpmlVIgt739GGPmA3eeUsIw9K/1+7l/yS4GpyVwz5zxRKjfXoRvnlPAG5vLeGLlXj4zOpuZQ9OcjtTrdPSkbQTQ9sJYF+BpbyNr7XxrravtFzCogxlC0pbSan6wcAMJ0W4euG4SSbFRTkcS6RGiIyO4e/Y43BEubl+4niNHm5yO1Ot0tOAXA9ltHmfxn+4e6aAjR30naeubW/ndF8dRkJHodCSRHmV0TjLfPm8opVUN/Pj5jZod6xR1tOCvBIwxpsAY4wauBhZ1Xazw09Dcyo2PrmbvoaPccs4QLhqd3f5GImHolnMKmJKfwqsby3hmdbHTcXqVDhV8a20DMA94FigEtgILuy5WePF4vNz29Ho+2H2IS8Zmc9v5pv2NRMKUO8LFvXPGkxgbyfyXNrPjQI3TkXqNU5oTz1qb3+b7xcC4rg4Ujn7x6hZe2VjK1EH9+N3scTpJK9KO3JR4fnPlWG5+fA03LVjDi7fM0BSfAdCdtg77+3u7ePD9Igoy+vC3aycTG+V2OpJIr/CZMdlcP3MQOw7U8l/PblB/fgBU8B30yoZSfvHqFjISY3j4K1NIjtcVOSKn4ocXD2dyXgovbyjl7+8VOR2nx1PBd8h72yv47lPriI9y84+vTCE3Jd7pSCK9TpQ7gvvmTiQzKYZfLtrCW4XlTkfq0VTwHbBs50FueGQVuOD+ayczqn+y05FEeq3MpFj+ft0UYiPdfOvJtWzeX+V0pB5LBb+brdxVyfUPr8LrhfuvnaS7BUW6wJjcZO6dM56jTa189eEP2XfoqNOReiQV/G60ZFsFX/7HBzS3erhv7kTO0YBoIl3motFZ3HHJCMqrG7n2wZUcqGlwOlKPo4LfTd7YXMYNj6zC44UHrpvE+SMznY4kEnJuOGMwt5wzhN2VR7nuwQ80/MJxVPC7wRMr93LTgtVEul08PG8K5w5XsRcJlu9fYJg7bSBby2q4+m8rOVSnon+MCn4Qeb1e/ve1rfz4+Y30jY/m8RumMb1AffYiweRyufjvy0Zz1dSBFJZW86UHlmt6RD8V/CCpbWzhGwvW8Od/7yQ/NZ7nb57OhIEpTscSCQsRES5+eflo5k3PZ1t5LV/46zKKDtY5HctxKvhBUHSwjsvvW8prm8uYNqgfz908g7zUBKdjiYQVl8vFnZeO5NZzC9hTeZQr/ryU1XsOOR3LUSr4XeyFtSVc+sf32X6glnnT81lwwzT6JUQ7HUskLLlcLm67wPCbK8dQ3dDCVQ+s5MkP9jodyzEabaiLVNU38/N/bea5NSUkRLv5/ZzxfH5CjtOxRASYM2Ug2clx3PrPtfzwuY2s2XuYuy4bHXZjV6mF3wUWbynngnvf5bk1JYzNTeaVb52hYi/Sw5w5LJ2Xb53J6Jwknl5VzKV/fJ+NxeF1V64KfieUHKnnlsfXcP0jqzhU18T3zh/Gwpumk5+m/nqRnmhAv3gW3jSdedPz2X6glsv/vJR739xGQ3Or09G6hbp0OqC2sYW/LdnF/Ut20tDsYcLAvvz6irGYLE1JKNLTxUa5mf+5UZw3IoPbn9nA/y3ezovrSpj/uVGcHeJ3v6vgn4KjTS08vmIvf3l3J4fqmkhPjOEXnx/O5RNyNGmJSC9zxtB03vzemdz75nYeWb6bef/4kDOGpnH7hYaxuX2djhcUKvgBOFDTwGPL9/DYij0cOdpMYmwkt50/jK/MHEQfzbIj0mslxkbxs0tH8oVJufxq0Rbe236Q97YfZNaITL5+1mAm56XgcoVOY07V6gRaWj0s3VnJkx/s5c3Cclo8XlLio/j2eUP5yox8+sbrUkuRUDGyfxKPXT+NZTsOcvcblre2lPPWlnLGDejL3KkD+ey4bOKje3+57P0/QRdqavGwavchXt9cxisbSzlY6xuDY3hWInNPy+MLE3OJiw6vy7hEwsn0gjSeHZLKqj2Huf/dXSzeWs76fUe46+VCLhiVySVjspk5NI2YyN5ZB8K64De1eNi0v4rVuw/z4e5DLN9ZSU1jCwD9EqK55rSBXDExlwkD+obUxzoROTGXy8WU/H5Mye9H8eGjPLOqmIWri3luTQnPrSkhLsrN5PwUpg9JY/qQVEbnJOPuJefwwqLge71eKmoa2VFRy86KOnYeqKVwfzXri4/Q2OL5aL0B/eK4clIu543I4LTBqUS5ddWqSDjLTYnnu+cP4zuzhrJ23xFe3VDKku0VH/X1AyTGRjImJ5lhmYkMz0pkWFYiwzITe+T5vU4lMsZcDdwBRAG/t9be1yWpArS1rJqdB+poaG6loaWVhmYP9U0tVNY1UVHTyMHaRipqGimvbqTW33I/xuWC4VlJTM5LYXJ+CpPz+5HTN64744tIL+FyuZg4MIWJ/gEQD9Q0sGLXIZbvPMjynZUs83+1lRwXRWZSDJlJsWQmxZKeGEOfmEjiotwkxLiJi45kRFYiQzO773LuDhd8Y0wO8AtgEtAILDPGvGOtLeyqcCfT3Orhsj8t/VgL/dOkxEeR0zeOQWkJDMlIYHBaH4Zk9GFIegKJsVHdEVVEQkxGYiyfG9efz43rD/juzdleXsO28hpsWS3bD9RQXt1AWVUD28prT7ifvvFRrP3p+d3WZdyZFv4s4G1r7SEAY8xC4AvAXae4HzdAWVnZKQe4a1Y2pVX1xES6iY1yEx0ZQWyUm5T4KFLio0lJiCbyU/vWaqk6WEt43VTdOUcqTv3/R3qv4mJ1Z56q9AhIz45gRnYSkPTR8oZmDwdrGzlc10R9cyv1zS00NHs42tTKgJR4SkpKTvm12tTLUzp73JmC3x8obfO4FJh6sg2MMfOBOz/tublz53Yiioh0pf9zOoAEKhvYGejKnSn4EYC3zWMXcNL+FWvtfGB+22XGmBhgCr43jJMNaFEEDOpAzlCmY/JxOh6fpGPySaFwTNz4iv2Hp7JRZwp+MXBGm8dZwP5T3Ym1thF4v731jDFYa3ef6v5DmY7Jx+l4fJKOySeF0DEJuGV/TGcK/lvAfGNMOlAHXAnc2In9iYhIEHX4zIy1tgT4CfAOsA54wlr7QVcFExGRrtWp6/CttU8AT3RRFhERCaLedO3Vz50O0APpmHycjscn6Zh8UtgeE5fX621/LRER6fV6UwtfREQ6QQVfRCRMqOCLiIQJFXwRkTChgi8iEiZU8EVEwkSPmJKlvYlUjDEGuB9IAcqAL1lrDxtjpgL3ATHAXuAGa22vH8f3ZMfDGDMeeLjN6unAYWvtaGPMQGABkAFYYK619sSDcfciHT0mbdb5b6DVP4BfSOjE78kM4F4gGqgEvmqt3dNtwYOkE8fjDOD3+I5HEfBla+3hbgvejRxv4beZSGUmMB640Rgzss3zLuAl4NfW2nHAWuCH/uULgR9Ya8cCjwIPdHf+rtbe8bDWrrPWjrfWjgemA4eBm/xP/xn4s7V2OLAK+Gm3hg+SzhwTY0yyMeZB4LbuTx48nfw9eRxf42i8//s/dGv4IOjk8fgHcK21dgxQCNzereG7keMFnzYTqVhr6/AV8S+0eX4iUGetfc3/+Jf4WvVpQJy19h3/8peBi/zDLfdm7R2Ptn4EvGutfd8YEwWc6V8ffK2Z2cEO2006dEz8jy8DtgO/C37MbtXR35MY4A5r7Qb/cxuAgcGPG3Sd+R0ZYa0t9P8N5eB7MwhJPaFLp72JVAqAMn8rbQKwBbgV339KnTHmAmvtG8CX8H2US6UDwzT3IAFNLGOMScY3OukY/6I0oNpa29Jmu9wg5uxOHT0mWGsf9T83P7gRu12Hjol/OPIF/uci8M1P8UKQs3aHzvyONBtjxuAbAbgZ+HFwozqnJ7Tw25tIJRI4G/iLtXYisAu4x1rrxTck84+NMWuBvvj6I5u6I3QQBTqxzDXAC9baAyfYjhNs1xt19JiEsk4dE2NMNL7unEh8n5p7u04dD2vtRmttJvDfwFNBS+mwnlDwi/HN3HLM8ROplAHbrbWr/I//yX/euZuttWdbayfga7W4gUNBzhts7R2PYz4PPNnm8QEg2RhzbI7L7BNs1xt19JiEsg4fE2NMH+A1fMX+Mmttc7BCdqMOHQ9jTKwx5vNtnl8AjA1Kwh6gJxT8t4DzjDHpxph4fK3219o8vwxIN8aM8z++FFjt//4fxpgp/u+/Bzxjre3trdr2jsexE9mTgOXHlvn/aN8D5vgXXQcs6pbEwdehYxLiOnNMFgA7gDn+Lp5Q0NHj0QzcZ4yZ5H/8RQKYga+3crzgn2giFWPMq8aYydbaeuBy4G/GmM3AufzniotvAPcbY7YCgwmBKzHaOx7+1dKBJmttw3Gb34zv6oRCfNNP3tFduYOpk8ckJHX0mBhjJuA7kT0DWGOMWWeMebWb43e5jh4Pa20rvkbSA8aYdfhO9N7Qvem7j4ZHFhEJE4638EVEpHuo4IuIhAkVfBGRMKGCLyISJlTwRUTChAq+iEiYUMEXEQkT/w/EiI84VSl/TAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ba90881f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating the dictionary of parameters to use in the search\n",
    "parameters = {'n_estimators': scipy.stats.randint(low=10, high=500),  # Uniform distribution between 10 and 500\n",
    "              'max_features': ['auto', 'log2', None],  # Number of features to consider at each split\n",
    "              'max_depth': [None, 10, 30, 100],  # Maximum number of levels in a tree\n",
    "              'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node\n",
    "              'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required at each leaf node\n",
    "             }\n",
    "\n",
    "hyperparameter_tune_get_results(random_forest, parameters, 'Random Forest', num_rounds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Show more directed hyperparameter tuning given current data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T17:25:18.838154Z",
     "start_time": "2018-07-04T17:24:59.089351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with 10 trees\n",
      "Fitting with 50 trees\n",
      "Fitting with 100 trees\n",
      "Fitting with 300 trees\n",
      "Fitting with 500 trees\n",
      "Fitting with 1000 trees\n"
     ]
    }
   ],
   "source": [
    "trees_to_try = [10, 50, 100, 300, 500, 1000]\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "for trees in trees_to_try:\n",
    "    print('Fitting with {0} trees'.format(trees))\n",
    "    random_forest = ensemble.RandomForestClassifier(n_estimators=trees, n_jobs=-1)\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    \n",
    "    train_accuracy.append(random_forest.score(X_train, y_train))\n",
    "    test_accuracy.append(random_forest.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T17:27:19.455851Z",
     "start_time": "2018-07-04T17:27:19.408978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumTrees</th>\n",
       "      <th>TestAccuracy</th>\n",
       "      <th>TrainAccuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.651333</td>\n",
       "      <td>0.982571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>0.998571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>0.694667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500</td>\n",
       "      <td>0.702000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NumTrees  TestAccuracy  TrainAccuracy\n",
       "0        10      0.651333       0.982571\n",
       "1        50      0.698000       0.998571\n",
       "2       100      0.694667       1.000000\n",
       "3       300      0.700000       1.000000\n",
       "4       500      0.702000       1.000000\n",
       "5      1000      0.700000       1.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plt.plot(trees_to_try, test_accuracy, label='Test Accuracy')\n",
    "# plt.plot(trees_to_try, train_accuracy, label='Train Accuracy')\n",
    "# plt.legend()\n",
    "\n",
    "pd.DataFrame({'NumTrees': trees_to_try, 'TrainAccuracy': train_accuracy, 'TestAccuracy': test_accuracy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees\n",
    "\n",
    "**TODO: Explain GBT tuning strategies & how different components affect the model**\n",
    "\n",
    "<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/07/Owen-Zhang-Table-of-Suggestions-for-Hyperparameter-Tuning-of-XGBoost.png\">\n",
    "\n",
    "**TODO: Adjust these hyperparameter ranges**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T17:29:47.478135Z",
     "start_time": "2018-07-04T17:27:53.860018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Parameters: \n",
      "\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False) \n",
      "\n",
      "Beginning hyperparameter tuning\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:   23.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:   27.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   31.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  15 | elapsed:   36.5s remaining:   24.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  15 | elapsed:   40.7s remaining:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  15 | elapsed:   44.6s remaining:   16.2s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:   50.3s remaining:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  15 | elapsed:  1.3min remaining:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  1.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Best estimator: \n",
      "\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.03, loss='deviance', max_depth=10,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=542,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "\n",
      "Accuracy before tuning: 0.723333333333\n",
      "Accuracy after tuning: 0.692666666667\n",
      "\n",
      " Tuned results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>LogLoss</th>\n",
       "      <th>AUC</th>\n",
       "      <th>TrainingTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.604539</td>\n",
       "      <td>0.743500</td>\n",
       "      <td>3.859142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.721333</td>\n",
       "      <td>0.600526</td>\n",
       "      <td>0.733113</td>\n",
       "      <td>47.309644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosted Trees</th>\n",
       "      <td>0.692667</td>\n",
       "      <td>0.737512</td>\n",
       "      <td>0.720375</td>\n",
       "      <td>112.946284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Accuracy   LogLoss       AUC  TrainingTime\n",
       "Logistic Regression     0.696000  0.604539  0.743500      3.859142\n",
       "Random Forest           0.721333  0.600526  0.733113     47.309644\n",
       "Gradient Boosted Trees  0.692667  0.737512  0.720375    112.946284"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ba90943978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating the dictionary of parameters to use in the search\n",
    "parameters = {'n_estimators': scipy.stats.randint(low=100, high=1000),  # Uniform distribution between 100 and 1000\n",
    "              'learning_rate': [0.01, 0.03, 0.1, 0.3],  # How drastic updates arelol\n",
    "              'max_depth': [None, 10, 30, 100],  # Maximum number of levels in a tree\n",
    "              'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node\n",
    "              'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required at each leaf node\n",
    "             }\n",
    "\n",
    "hyperparameter_tune_get_results(gradient_boosting, parameters, 'Gradient Boosted Trees', num_rounds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Show more directed hyperparameter tuning given current data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "**TODO: Talk about how it's more ambiguous due to being a collection of several models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Frameworks\n",
    "\n",
    "We have been using scikit-learn up until now for our models, but there are more specialized frameworks for gradient boosting in particular. Scikit-learn's gradient boosting algorithm is good, but lacks a few components and options that can be useful.\n",
    "\n",
    "Specifically, we're going to focus on **XGBoost** and **LightGBM**. We'll go into more specifics for each, but both frameworks have the following advantages & disadvantages:\n",
    "\n",
    "#### Advantages\n",
    "- Ability to parallelize training\n",
    "- Ability to use GPUs\n",
    "- Additional under-the-hood optimization\n",
    "- Can specify loss functions\n",
    "- Additional tuning parameters\n",
    "- Distributed computing options\n",
    "\n",
    "#### Disadvantages\n",
    "- Relatively difficult to install\n",
    "- Not as unified integration\n",
    "\n",
    "So generally speaking, XGBoost and LightGBM is able to train better models faster, but can be more difficult to set up and use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "**TODO: Explain XGBoost, what it is, how it works, and why it's good**\n",
    "\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/11194110/leaf.png\">\n",
    "\n",
    "*Source: [LightGBM's GitHub](https://github.com/Microsoft/LightGBM/blob/master/docs/Features.rst)*\n",
    "\n",
    "#### Installation\n",
    "**TODO: Explain how to install xgboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "\n",
    "[LightGBM](https://github.com/Microsoft/LightGBM) is a project from [Microsoft Research Asia](https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/) that is focused around training gradient boosted trees in a highly efficient and distributed manner. It's generally comparable to XGBoost, but is not as popular because it is much newer.\n",
    "\n",
    "One of the fundamental differences between LightGBM and other implementations of gradient boosted trees is that it grows the trees leaf-wise rather than level-wise:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Microsoft/LightGBM/master/docs/_static/images/leaf-wise.png\">\n",
    "\n",
    "*Source: [LightGBM's GitHub](https://github.com/Microsoft/LightGBM/blob/master/docs/Features.rst)*\n",
    "\n",
    "**TODO: Explain why this is important**\n",
    "\n",
    "Additionally, LightGBM uses a histogram based algorithm to discretize continuous variables into buckets in order to speed up the training process and reduce the memory requirements. XGBoost has included this in a recent version, but it is not enabled by default. **TODO: Validate this**\n",
    "\n",
    "There are several other optimizations happening under the hood, but those are a few of the main differences from other implementations.\n",
    "\n",
    "#### Installation\n",
    "\n",
    "[The documentation on GitHub](https://github.com/Microsoft/LightGBM/tree/master/python-package#installation) has installation instructions for LightGBM. It can be installed from PyPI with `pip install lightgbm`, but requires a few things to work - check out the documentation depending on your OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T17:32:47.545358Z",
     "start_time": "2018-07-04T17:32:47.420368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model\n",
      "Completed\n",
      "\n",
      " Non-tuned results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>LogLoss</th>\n",
       "      <th>AUC</th>\n",
       "      <th>TrainingTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.604543</td>\n",
       "      <td>0.743498</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.655333</td>\n",
       "      <td>1.374621</td>\n",
       "      <td>0.693786</td>\n",
       "      <td>0.187489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosted Trees</th>\n",
       "      <td>0.723333</td>\n",
       "      <td>0.596558</td>\n",
       "      <td>0.733223</td>\n",
       "      <td>0.906195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stacking</th>\n",
       "      <td>0.650667</td>\n",
       "      <td>1.065496</td>\n",
       "      <td>0.679866</td>\n",
       "      <td>6.265244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.722667</td>\n",
       "      <td>0.600648</td>\n",
       "      <td>0.734010</td>\n",
       "      <td>0.046872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Accuracy   LogLoss       AUC  TrainingTime\n",
       "Logistic Regression     0.696000  0.604543  0.743498      0.015625\n",
       "Random Forest           0.655333  1.374621  0.693786      0.187489\n",
       "Gradient Boosted Trees  0.723333  0.596558  0.733223      0.906195\n",
       "Stacking                0.650667  1.065496  0.679866      6.265244\n",
       "LightGBM                0.722667  0.600648  0.734010      0.046872"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lightGBM = lgb.LGBMClassifier(nthread=-1)  # nthread=-1 uses all available cores\n",
    "\n",
    "# Due to the scikit-learn API option, LightGBM works with our function!\n",
    "train_model_get_results(lightGBM, 'LightGBM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T17:36:11.655811Z",
     "start_time": "2018-07-04T17:34:04.886Z"
    }
   },
   "outputs": [],
   "source": [
    "# parameters = {'n_estimators': scipy.stats.randint(low=10, high=500),  # Uniform distribution between 10 and 750\n",
    "#               'max_features': ['auto', 'sqrt'],  # Number of features to consider at each split\n",
    "#               'max_depth': [None, 10, 30, 100],  # Maximum number of levels in a tree\n",
    "#               'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node\n",
    "#               'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required at each leaf node\n",
    "# #               'boostrap': [True, False]\n",
    "#              }  # If the samples should be bootstrapped or not\n",
    "             \n",
    "parameters = {'boosting_type': ['gbdt', 'dart'],\n",
    "              'n_estimators': scipy.stats.randint(low=50, high=5000),  # Uniform distribution between 10 and 1000\n",
    "              'learning_rate': [0.01, 0.03, 0.1, 0.3],\n",
    "#               'num_leaves': 31,\n",
    "#               'max_depth': 1,\n",
    "#               'max_bin': 255,\n",
    "#               'reg_alpha': 0,  # L1 regularization\n",
    "#               'reg_lambda': 0  # L2 regularization\n",
    "             }    \n",
    "\n",
    "hyperparameter_tune_get_results(lightGBM, parameters, 'LightGBM', num_rounds=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
